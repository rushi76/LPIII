# ------------------------------------------------------------
# PRACTICAL 1 : PREDICT UBER RIDE PRICE
# ------------------------------------------------------------

# ğŸ¯ OBJECTIVE:
# Predict Uber fare from pickup and drop-off points using
# Linear Regression and Random Forest Regression.
# Compare performance using RÂ² and RMSE metrics.
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 1 : INSTALL REQUIRED LIBRARIES
# ------------------------------------------------------------
import sys
!{sys.executable} -m pip install pandas numpy matplotlib seaborn scikit-learn haversine --quiet
print("âœ… All libraries installed successfully.")
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 2 : IMPORT LIBRARIES & LOAD DATASET
# ------------------------------------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from haversine import haversine
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

# Load dataset (make sure 'uber.csv' is in same folder)
df = pd.read_csv("uber.csv")
print("âœ… Dataset loaded successfully.")
df.head()
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 3 : EXPLORE & CLEAN DATA
# ------------------------------------------------------------
print("Shape:", df.shape)
print(df.info())
print("Missing values:\n", df.isnull().sum())

# Drop unnecessary columns if present
for col in ['Unnamed: 0', 'key']:
    if col in df.columns:
        df.drop(col, axis=1, inplace=True)

# Drop null rows
df = df.dropna()
print("âœ… After removing null values:", df.shape)
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 4 : CONVERT DATETIME â†’ EXTRACT FEATURES
# ------------------------------------------------------------
if 'pickup_datetime' in df.columns:
    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], errors='coerce')
    df['hour'] = df['pickup_datetime'].dt.hour
    df['day'] = df['pickup_datetime'].dt.day
    df['month'] = df['pickup_datetime'].dt.month
    df['year'] = df['pickup_datetime'].dt.year
    df['dayofweek'] = df['pickup_datetime'].dt.dayofweek
    df.drop('pickup_datetime', axis=1, inplace=True)

df = df.dropna().reset_index(drop=True)
print("âœ… Datetime converted and features extracted.")
df.head()
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 5 : REMOVE INVALID ENTRIES & OUTLIERS
# ------------------------------------------------------------
# Remove negative or zero fares
df = df[df['fare_amount'] > 0]

# Keep valid passenger count
df = df[(df['passenger_count'] > 0) & (df['passenger_count'] < 8)]

# IQR method for fare_amount
Q1 = df['fare_amount'].quantile(0.25)
Q3 = df['fare_amount'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
df['fare_amount'] = df['fare_amount'].clip(lower, upper)
print("âœ… Outliers handled using IQR method.")
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 6 : FEATURE ENGINEERING - CALCULATE DISTANCE
# ------------------------------------------------------------
distances = []
for i, row in df.iterrows():
    loc1 = (row['pickup_latitude'], row['pickup_longitude'])
    loc2 = (row['dropoff_latitude'], row['dropoff_longitude'])
    distances.append(haversine(loc1, loc2))

df['distance_km'] = distances

# Filter unrealistic distances
df = df[(df['distance_km'] >= 0.1) & (df['distance_km'] <= 130)]
print("âœ… Distance calculated using Haversine formula.")
df[['distance_km','fare_amount']].head()
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 7 : CORRELATION ANALYSIS
# ------------------------------------------------------------
plt.figure(figsize=(10,6))
sns.heatmap(df.select_dtypes(include=['float64','int64']).corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Matrix")
plt.show()
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 8 : SPLIT DATA INTO TRAIN & TEST SETS
# ------------------------------------------------------------
features = ['pickup_longitude','pickup_latitude','dropoff_longitude',
            'dropoff_latitude','passenger_count','hour','day',
            'month','year','dayofweek','distance_km']

# Keep only existing columns (some datasets may vary)
features = [f for f in features if f in df.columns]

X = df[features]
y = df['fare_amount']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("âœ… Train/Test split done.")
print("Training shape:", X_train.shape, "Testing shape:", X_test.shape)
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 9 : TRAIN LINEAR REGRESSION MODEL
# ------------------------------------------------------------
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

r2_lr = r2_score(y_test, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
print("Linear Regression â†’ RÂ²:", round(r2_lr,4), " RMSE:", round(rmse_lr,4))
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 10 : TRAIN RANDOM FOREST REGRESSION MODEL
# ------------------------------------------------------------
rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

r2_rf = r2_score(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
print("Random Forest â†’ RÂ²:", round(r2_rf,4), " RMSE:", round(rmse_rf,4))
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 11 : COMPARE MODEL PERFORMANCE
# ------------------------------------------------------------
print("=== MODEL COMPARISON ===")
print(f"Linear Regression : RÂ² = {r2_lr:.3f}, RMSE = {rmse_lr:.3f}")
print(f"Random Forest     : RÂ² = {r2_rf:.3f}, RMSE = {rmse_rf:.3f}")

models = ['Linear Regression','Random Forest']
r2_scores = [r2_lr, r2_rf]
rmse_scores = [rmse_lr, rmse_rf]

plt.figure(figsize=(6,3))
sns.barplot(x=models, y=r2_scores)
plt.title("Model Comparison - RÂ² Score")
plt.ylabel("RÂ²")
plt.ylim(0,1)
plt.show()

plt.figure(figsize=(6,3))
sns.barplot(x=models, y=rmse_scores)
plt.title("Model Comparison - RMSE")
plt.ylabel("RMSE")
plt.show()
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 12 : OPTIONAL - FEATURE IMPORTANCE (RANDOM FOREST)
# ------------------------------------------------------------
fi = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
print("Feature Importance (Random Forest):")
print(fi)

plt.figure(figsize=(6,3))
sns.barplot(x=fi.values, y=fi.index)
plt.title("Feature Importance")
plt.show()
# ------------------------------------------------------------


# ------------------------------------------------------------
# âœ… END OF PRACTICAL 1
# ------------------------------------------------------------


Q1. What is the objective of this practical?
ğŸ‘‰ To predict Uber ride fare using machine learning regression models.

Q2. What preprocessing steps did you perform?
ğŸ‘‰ Removed nulls, dropped unnecessary columns, extracted date-time features, and removed outliers.

Q3. Why use the Haversine formula?
ğŸ‘‰ It accurately calculates distance between two geographical coordinates on Earthâ€™s surface.

Q4. Which models did you use and why?
ğŸ‘‰ Linear Regression (simple baseline) and Random Forest (non-linear, more accurate).

Q5. Which metrics did you use to evaluate?
ğŸ‘‰ RÂ² score and RMSE (Root Mean Squared Error).

Q6. What result did you get?
ğŸ‘‰ Random Forest showed a higher RÂ² and lower RMSE â†’ better performance.

Q7. Why Random Forest performs better?
ğŸ‘‰ It captures non-linear relationships and feature interactions.

Q8. What features did you use?
ğŸ‘‰ Coordinates, passenger_count, extracted time features, and calculated distance_km.

Q9. How can you improve accuracy?
ğŸ‘‰ Add features like weather, traffic, surge pricing, and tune model hyperparameters.

Q10. Final Conclusion:
ğŸ‘‰ Random Forest Regression gives better prediction of Uber fares than Linear Regression.
